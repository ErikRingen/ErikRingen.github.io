{
  "hash": "9b64105e4ab1fc054d807e7429c2495b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"The right way to do predictive checks with observation-level random effects\"\nauthor: \"Erik J. Ringen\"\ndate: \"2024-11-10\"\ncategories: [Prediction, Statistics]\nimage: \"image.jpg\"\n---\n\n\n\nObservation-level random effects (OLRE) are a useful way to model overdispersed count data. For example, OLREs relax the assumption of Poisson regression that the variance is equal to the mean by adding a \"random intercept\" ($\\nu$) for every observation:\n\n$$ y_i \\sim \\text{Poisson}(\\lambda_i)$$\n$$ \\text{log}(\\lambda_i) = b_0 + \\nu_i$$\n$$ \\nu_i \\sim \\mathcal{N} (0, \\sigma_{\\text{OLRE}}) $$\n\nBy putting $\\nu$ inside the linear model, we smuggle a variance term into a distribution that otherwise has only a single rate parameter (this trick also works for the binomial distribution). OLREs are also useful for [multi-response models](https://cran.r-project.org/web/packages/brms/vignettes/brms_multivariate.html), allowing us to capture residual correlations between distributions that are not multivariate normal or multivariate student-t without resorting to [copulas](https://en.wikipedia.org/wiki/Copula_(statistics)). Unfortunately, **most software for performing predictive checks from such models will do so the wrong way by default.**\n\n# The wrong way\n\nA basic form of model checking in Bayesian workflow is the [posterior predictive check](https://mc-stan.org/docs/stan-users-guide/posterior-predictive-checks.html). In brief, we use samples from our model to generate many synthetic replications of our data, denoted $y_{\\text{rep}}$, and then compare these replications to the observed data. Systematic discrepancies between the distribution of $y_\\text{rep}$ and the real data indicate misspecification, and can suggest ways to improve our model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brms)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(bayesplot)\nlibrary(tidybayes)\nlibrary(patchwork)\ndata(Kline, package = \"rethinking\")\n\nKline$log_pop_z <- scale(log(Kline$population))\n\nm_poisson <- brm(\n    total_tools ~ 1 + log_pop_z,\n    family = poisson(link = \"log\"),\n    prior = prior(normal(3, 0.5), class = \"Intercept\") + \n        prior(normal(0, 0.2), class = \"b\"),\n    data = Kline)\n\ncolor_scheme_set(\"teal\")\ntheme_set(theme_classic(base_size = 13))\n\nbrms::pp_check(m_poisson, type = \"dens_overlay\", ndraws = 100) + theme(legend.position = \"none\") + brms::pp_check(m_poisson, type = \"intervals\") + plot_layout(guides = 'collect') + theme_classic(base_size = 13)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\nWe have some clear indicators that the data are overdispersed, relative to the model predictions. OK, lets try adding an OLRE.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_poisson_OLRE <- brm(\n    total_tools ~ 1 + log_pop_z + (1|culture),\n    family = poisson(link = \"log\"),\n    prior = prior(normal(3, 0.5), class = \"Intercept\") + \n        prior(normal(0, 0.2), class = \"b\") + \n        prior(exponential(2), class = \"sd\"),\n    control = list(adapt_delta = 0.95),\n    data = Kline,\n    save_pars = save_pars(all = TRUE))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n\n```{.r .cell-code}\nbrms::pp_check(m_poisson_OLRE, type = \"dens_overlay\", ndraws = 100) + theme(legend.position = \"none\") + brms::pp_check(m_poisson_OLRE, type = \"intervals\") +\n    plot_annotation(subtitle = \"Wrong way\") \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'intervals' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\nLooks good, right? Sadly, this is a little too good to be true. We have mislead ourselves, and to see why, have a look at the $\\nu_{\\text{culture}}$ parameters in comparison to the observed data:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\nPlot X shows us that the OLREs are positively correlated with the values of the raw data. Why? These parameters are doing exactly what they are supposed to do: capture variance in the data to account for excess dispersion. But if you think for a moment about *out-of-sample* prediction (generating $y_\\text{rep}$ for a new culture with some unknown $y_{\\text{test}}$), you might intuit the problem. $y_{\\text{test}}$ will not be known ahead of time, and thus the OLRE can convey no information about the value $y_{\\text{test}}$. \n\n# The right way\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyrep_OLRE <- m_poisson_OLRE |> \n    posterior_predict(newdata = Kline |> \n    mutate(culture = paste(\"OLRE_rep\", 1:n())),\n     allow_new_levels = TRUE)\n\nbayesplot::ppc_dens_overlay(Kline$total_tools, yrep_OLRE[1:100,]) + theme(legend.position = \"none\") + bayesplot::ppc_intervals(Kline$total_tools, yrep_OLRE) +\n    plot_annotation(subtitle = \"Right way\") \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\nNotice that, unlike our first predictive check with no OLRE, the credible intervals of $y_{\\text{rep}}$ all contain the observed values of y. But unlike our (wrong) second predictive check, the predictions do not conform so closely to the true values. Now, we get a realistic picture of our model's behavior--with the caveat that any posterior predictive checks will be overly optimistic for out-of-sample data because we are using the same data points to both fit and evaluate the model. So a more complex model that looks better in a posterior predictive check might actually look worse when generalizing to new data, i.e., we overfit.\n\n# An even better way\n\nUsing an approximation to leave-one-out cross validation (LOO-CV), we can construct predictions where one data point at a time is left out of model fitting and then used as a test point. This provides a more honest appraisal of our model's predictive ability, because we don't double-dip.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(loo)\nloo_OLRE <- loo(m_poisson_OLRE, save_psis = TRUE, moment_match = TRUE)\n\nbayesplot::ppc_loo_intervals(Kline$total_tools, yrep_OLRE, loo_OLRE$psis_object) +\n     labs(subtitle = \"Right way + PSIS LOO\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nThe differences with the previous plot are pretty subtle, suggesting that our model hasn't overfit very much.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}